{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:00.064001Z",
     "start_time": "2023-02-27T18:16:53.662325Z"
    },
    "execution": {
     "iopub.execute_input": "2021-06-13T17:01:18.730631Z",
     "iopub.status.busy": "2021-06-13T17:01:18.730276Z",
     "iopub.status.idle": "2021-06-13T17:01:20.535109Z",
     "shell.execute_reply": "2021-06-13T17:01:20.534047Z",
     "shell.execute_reply.started": "2021-06-13T17:01:18.730555Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find plotting backend 'pandas_bokeh'. Ensure that you've installed the package providing the 'pandas_bokeh' entrypoint, or that the package has a top-level `.plot` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplotting.backend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpandas_bokeh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas_bokeh\u001b[39;00m\n\u001b[1;32m     23\u001b[0m pandas_bokeh\u001b[38;5;241m.\u001b[39moutput_notebook()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/_config/config.py:263\u001b[0m, in \u001b[0;36mCallableDynamicDoc.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/_config/config.py:160\u001b[0m, in \u001b[0;36m_set_option\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m o \u001b[38;5;241m=\u001b[39m _get_registered_option(key)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m o \u001b[38;5;129;01mand\u001b[39;00m o\u001b[38;5;241m.\u001b[39mvalidator:\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# walk the nested dict\u001b[39;00m\n\u001b[1;32m    163\u001b[0m root, k \u001b[38;5;241m=\u001b[39m _get_root(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/config_init.py:751\u001b[0m, in \u001b[0;36mregister_plotting_backend_cb\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_plot_backend\n\u001b[0;32m--> 751\u001b[0m \u001b[43m_get_plot_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/plotting/_core.py:1886\u001b[0m, in \u001b[0;36m_get_plot_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend_str \u001b[38;5;129;01min\u001b[39;00m _backends:\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _backends[backend_str]\n\u001b[0;32m-> 1886\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43m_load_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1887\u001b[0m _backends[backend_str] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/plotting/_core.py:1855\u001b[0m, in \u001b[0;36m_load_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1851\u001b[0m         \u001b[38;5;66;03m# Validate that the interface is implemented when the option is set,\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m         \u001b[38;5;66;03m# rather than at plot time.\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m module\n\u001b[0;32m-> 1855\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find plotting backend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Ensure that you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstalled the package providing the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m entrypoint, or that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe package has a top-level `.plot` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1859\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find plotting backend 'pandas_bokeh'. Ensure that you've installed the package providing the 'pandas_bokeh' entrypoint, or that the package has a top-level `.plot` method."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        continue\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "pd.set_option('plotting.backend', 'pandas_bokeh')\n",
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()\n",
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:00.070332Z",
     "start_time": "2023-02-27T18:17:00.070308Z"
    }
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# login_str = 'login_haj'\n",
    "# module = importlib.import_module(login_str, package=None)\n",
    "# account_name = login_str\n",
    "# print('Logging in with account : '+str(account_name))\n",
    "# module.retry_autologin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:00.075461Z",
     "start_time": "2023-02-27T18:17:00.075443Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.io import export_png\n",
    "\n",
    "user_id = 'YOUR_ZERODHA_USER_ID'\n",
    "password = 'YOUR_ZERODHA_PASSWORD'  # Replace with your password\n",
    "totp_key = 'YOUR_ZERODHA_2FA_KEY'   # Replace with your TOTP secret key\n",
    "\n",
    "# Generate the TOTP code for two-factor authentication\n",
    "totp = pyotp.TOTP(totp_key)\n",
    "twofa = totp.now()\n",
    "\n",
    "# Initialize DataLoader and authenticate\n",
    "data_loader = DataLoader()\n",
    "kite = data_loader.authenticate(user_id=user_id, password=password, twofa=twofa)\n",
    "\n",
    "kite.profile()['user_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:27.699538Z",
     "start_time": "2023-02-27T18:17:27.613629Z"
    },
    "execution": {
     "iopub.execute_input": "2021-06-13T17:01:20.599785Z",
     "iopub.status.busy": "2021-06-13T17:01:20.599116Z",
     "iopub.status.idle": "2021-06-13T17:01:20.733433Z",
     "shell.execute_reply": "2021-06-13T17:01:20.732410Z",
     "shell.execute_reply.started": "2021-06-13T17:01:20.599741Z"
    }
   },
   "outputs": [],
   "source": [
    "input_window = 300\n",
    "output_window = 5\n",
    "batch_size = 10 # batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=30,num_layers=2,dropout=0.2):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "\n",
    "def get_data2(inst):\n",
    "    global scaler\n",
    "    old_lst=[]\n",
    "    interval='5minute'\n",
    "    todaydt=datetime.date.today()\n",
    "    hud_ago=todaydt-datetime.timedelta(days=50) #59\n",
    "    to_date=datetime.date.isoformat(todaydt)\n",
    "    from_date=datetime.date.isoformat(hud_ago)\n",
    "\n",
    "    for i2 in range(1):\n",
    "        new_lst = module.kite.historical_data(inst, from_date, to_date, interval,continuous=False)\n",
    "        old_lst = new_lst + old_lst\n",
    "        todaydt=todaydt-datetime.timedelta(days=51) #60\n",
    "        hud_ago=hud_ago-datetime.timedelta(days=51) #60\n",
    "        to_date=datetime.date.isoformat(todaydt)\n",
    "        from_date=datetime.date.isoformat(hud_ago)\n",
    "    df=pd.DataFrame(old_lst)\n",
    "    df_nifty = df\n",
    "    this_inst_df = df_nifty\n",
    "    amplitude = this_inst_df['close'].to_numpy()[-905:]\n",
    "    amplitude = amplitude.reshape(-1)\n",
    "    scaler = MinMaxScaler(feature_range=(-15, 15)) \n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    sampels = int(amplitude.shape[0]*0)\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude\n",
    "    train_sequence = create_inout_sequences(train_data,input_window)\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "    test_data = create_inout_sequences(test_data,input_window)\n",
    "    test_data = test_data[:-output_window]\n",
    "    return train_sequence.to(device),test_data.to(device)\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).to(device).item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).to(device).item()            \n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "plot_counter = 0\n",
    "def plot_and_loss(eval_model, data_source,epoch,tknip):\n",
    "    global plot_counter\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "\n",
    "            test_result = torch.cat((test_result.to(device), output[-1].view(-1).to(device)), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth.to(device), target[-1].view(-1).to(device)), 0)\n",
    "    test_result = test_result.cpu().numpy()\n",
    "    truth = truth.cpu().numpy()\n",
    "    len(test_result)\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "def predict_future_open(eval_model, data_source,steps,tkn):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "    data = data.cpu().view(-1)\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    return data\n",
    "\n",
    "def predict_future(eval_model, data_source,steps,tkn):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "    data = data.cpu().view(-1)\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig(f'./nmnm/transformer-future_{plot_counter}_{steps}_{tkn}.png')\n",
    "    pyplot.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:06.569313Z",
     "start_time": "2023-02-27T18:17:06.564444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:04.503909Z",
     "start_time": "2023-02-27T18:17:04.498060Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install torch==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:17:30.214153Z",
     "start_time": "2023-02-27T18:17:30.115336Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_LinearWithBias' on <module 'torch.nn.modules.linear' from '/home/hemang/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./best_model_multi8.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfind_class(mod_name, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute '_LinearWithBias' on <module 'torch.nn.modules.linear' from '/home/hemang/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py'>"
     ]
    }
   ],
   "source": [
    "model= torch.load('./best_model_multi8.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T11:43:44.668487Z",
     "start_time": "2021-06-20T11:43:44.665872Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data, val_data = get_data2(2029825)\n",
    "# predict_future(model,val_data,2000,2029825)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T11:43:44.692422Z",
     "start_time": "2021-06-20T11:43:44.688092Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look_up = 1001\n",
    "\n",
    "# inst_check_list = [1793,5633,6401,3861249,2995969,25601,325121,6483969,40193,41729,54273,\n",
    "#                    60417,5436929,70401,1510401,4267265,4268801]\n",
    "\n",
    "# for one in tqdm(inst_check_list):\n",
    "#     train_data, val_data = get_data2(one)\n",
    "#     col_list = []\n",
    "\n",
    "#     orig_data = np.array([])\n",
    "#     orig_data\n",
    "\n",
    "#     for one_part_point in range(15):   #  total_parts\n",
    "#     #     print(val_data[-(300*(one_part_point+1))::].shape)\n",
    "#         dpp = predict_future_open(model, val_data[-(300*(one_part_point+1))::],2000,123123)\n",
    "\n",
    "#         col_list.append(np.append(orig_data,dpp))\n",
    "\n",
    "#         orig_data = np.append(orig_data,dpp[:input_window])\n",
    "\n",
    "#     #     col_list.append(dpp)\n",
    "#     col_list.append(orig_data)\n",
    "#     pyplot.savefig(f'./nmnm/test_plot.png')\n",
    "#     pyplot.close()\n",
    "\n",
    "#     plot_df = pd.DataFrame(col_list)\n",
    "#     trps = plot_df.transpose()\n",
    "#     trps.plot()\n",
    "#     pd.DataFrame(orig_data).plot()\n",
    "    \n",
    "    \n",
    "# #     predict_future(model,val_data,look_up,one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T03:45:11.226541Z",
     "start_time": "2021-06-16T03:45:09.970313Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:18:27.314530Z",
     "start_time": "2023-02-27T18:18:27.265580Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m test_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m one_part_point \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(test_len)):   \u001b[38;5;66;03m#  total_parts\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     dpp \u001b[38;5;241m=\u001b[39m predict_future_open(\u001b[43mmodel\u001b[49m, val_data[input_window\u001b[38;5;241m*\u001b[39m(one_part_point):input_window\u001b[38;5;241m*\u001b[39m(one_part_point\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)],\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m123123\u001b[39m)\n\u001b[1;32m      8\u001b[0m     col_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mappend(orig_data,dpp))\n\u001b[1;32m      9\u001b[0m     orig_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(orig_data,dpp[:input_window])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "col_list = []\n",
    "orig_data = np.array([])\n",
    "test_len = 15\n",
    "\n",
    "\n",
    "for one_part_point in tqdm(range(test_len)):   #  total_parts\n",
    "    dpp = predict_future_open(model, val_data[input_window*(one_part_point):input_window*(one_part_point+1)],1000,123123)\n",
    "    col_list.append(np.append(orig_data,dpp))\n",
    "    orig_data = np.append(orig_data,dpp[:input_window])\n",
    "\n",
    "col_list.append(orig_data)\n",
    "pyplot.savefig(f'./nmnm/test_plot.png')\n",
    "pyplot.close()\n",
    "\n",
    "plot_df = pd.DataFrame(col_list)\n",
    "trps = plot_df.transpose()\n",
    "trps.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:18:29.834597Z",
     "start_time": "2023-02-27T18:18:29.830115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for jj in range(8):\n",
    "#     print(jj+1)\n",
    "#     model= torch.load(f'./best_model_multi{jj+1}.pt',map_location=torch.device('cpu'))\n",
    "    \n",
    "#     col_list = []\n",
    "#     orig_data = np.array([])\n",
    "#     test_len = 8\n",
    "\n",
    "#     for one_part_point in tqdm(range(test_len)):   #  total_parts\n",
    "#         dpp = predict_future_open(model, val_data[input_window*(one_part_point):input_window*(one_part_point+1)],100,123123)\n",
    "#         col_list.append(np.append(orig_data,dpp))\n",
    "#         orig_data = np.append(orig_data,dpp[:input_window])\n",
    "\n",
    "#     col_list.append(orig_data)\n",
    "#     pyplot.savefig(f'./nmnm/test_plot.png')\n",
    "#     pyplot.close()\n",
    "\n",
    "#     plot_df = pd.DataFrame(col_list)\n",
    "#     trps = plot_df.transpose()\n",
    "#     trps.plot()\n",
    "#     print('*'*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:18:30.867780Z",
     "start_time": "2023-02-27T18:18:30.789509Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3356417\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 69\u001b[0m, in \u001b[0;36mget_data2\u001b[0;34m(inst)\u001b[0m\n\u001b[1;32m     66\u001b[0m from_date\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39misoformat(hud_ago)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 69\u001b[0m     new_lst \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241m.\u001b[39mkite\u001b[38;5;241m.\u001b[39mhistorical_data(inst, from_date, to_date, interval,continuous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m     old_lst \u001b[38;5;241m=\u001b[39m new_lst \u001b[38;5;241m+\u001b[39m old_lst\n\u001b[1;32m     71\u001b[0m     todaydt\u001b[38;5;241m=\u001b[39mtodaydt\u001b[38;5;241m-\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m51\u001b[39m) \u001b[38;5;66;03m#60\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'module' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data2(3356417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T18:18:32.804631Z",
     "start_time": "2023-02-27T18:18:32.779997Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval_data\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "val_data[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T06:51:14.766047Z",
     "start_time": "2021-06-21T06:50:46.100703Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, val_data = get_data2(3529217)\n",
    "col_list = []\n",
    "orig_data = np.array([])\n",
    "test_len = 2\n",
    "model= torch.load(f'./best_model_multi18.pt',map_location=torch.device('cpu'))\n",
    "for one_part_point in tqdm(range(test_len)):   #  total_parts\n",
    "    dpp = predict_future_open(model, val_data[input_window*(one_part_point):input_window*(one_part_point+1)],\n",
    "                              1000,123123)\n",
    "    mod = dpp[0].numpy()\n",
    "    if (orig_data.size != 0): #check not empty\n",
    "        org = orig_data[-1]\n",
    "        diff = org-mod\n",
    "        dpp = dpp + diff\n",
    "\n",
    "    col_list.append(np.append(orig_data,dpp))\n",
    "    orig_data = np.append(orig_data,dpp[:input_window])\n",
    "\n",
    "pyplot.savefig(f'./nmnm/test_plot.png')\n",
    "pyplot.close()\n",
    "\n",
    "plot_df = pd.DataFrame(col_list)\n",
    "trps = plot_df.transpose()\n",
    "trps.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T06:58:47.728113Z",
     "start_time": "2021-06-21T06:58:11.353991Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, val_data = get_data2(3356417)  #3529217\n",
    "col_list = []\n",
    "orig_data = np.array([])\n",
    "test_len = 2\n",
    "model= torch.load(f'./best_model_multi18.pt',map_location=torch.device('cpu'))\n",
    "for one_part_point in tqdm(range(test_len)):   #  total_parts\n",
    "    dpp = predict_future_open(model, val_data[input_window*(one_part_point):input_window*(one_part_point+1)],\n",
    "                              1000,123123)\n",
    "    if (orig_data.size != 0): #check not empty\n",
    "        diff = orig_data[-1] - dpp[301].numpy()\n",
    "        dpp = dpp - diff\n",
    "\n",
    "    col_list.append(np.append(orig_data,dpp))\n",
    "    orig_data = np.append(orig_data,dpp[:input_window])\n",
    "\n",
    "pyplot.savefig(f'./nmnm/test_plot.png')\n",
    "pyplot.close()\n",
    "\n",
    "plot_df = pd.DataFrame(col_list)\n",
    "trps = plot_df.transpose()\n",
    "trps.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T01:53:05.537796Z",
     "start_time": "2021-06-18T01:42:10.655185Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(14,19):\n",
    "    for jj in [1459457,70401,261889,]:\n",
    "#     for jj in [3861249,6401,3677697,3669505]:\n",
    "        print('*'*50)\n",
    "        print(i)\n",
    "        print(jj)\n",
    "        \n",
    "        train_data, val_data = get_data2(jj)\n",
    "        col_list = []\n",
    "        orig_data = np.array([])\n",
    "        test_len = 6\n",
    "        model= torch.load(f'./best_model_multi{i}.pt',map_location=torch.device('cpu'))\n",
    "        for one_part_point in tqdm(range(test_len)):   #  total_parts\n",
    "            dpp = predict_future_open(model, val_data[input_window*(one_part_point):input_window*(one_part_point+1)],\n",
    "                                      300,123123)\n",
    "            mod = dpp[0].numpy()\n",
    "            if (orig_data.size != 0): #check not empty\n",
    "                org = orig_data[-1]\n",
    "                diff = org-mod\n",
    "                dpp = dpp + diff\n",
    "\n",
    "            col_list.append(np.append(orig_data,dpp))\n",
    "            orig_data = np.append(orig_data,dpp[:input_window])\n",
    "\n",
    "        pyplot.savefig(f'./nmnm/test_plot.png')\n",
    "        pyplot.close()\n",
    "\n",
    "        plot_df = pd.DataFrame(col_list)\n",
    "        trps = plot_df.transpose()\n",
    "        trps.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T04:15:42.695654Z",
     "start_time": "2021-06-19T04:15:22.902983Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df = pd.read_excel('../valid_loss_map_df_5x (17).xlsx')\n",
    "loss_df['name'] =''\n",
    "type(loss_df['inst'][0])\n",
    "all_inst = pd.read_excel('./all_inst.xlsx')\n",
    "df3 = pd.merge(loss_df,all_inst,left_on=['inst'], right_on = ['instrument_token'], how = 'left')\n",
    "df3['ltp']=0.0\n",
    "df3 = df3[0:143]\n",
    "inedx_counter = 0\n",
    "for one_symbol in tqdm(df3.tradingsymbol):\n",
    "    ltp = module.kite.quote([f'NSE:{one_symbol}'])[f'NSE:{one_symbol}']['last_price']\n",
    "    df3.at[inedx_counter, 'ltp'] = ltp\n",
    "#     print(one_symbol)    \n",
    "#     print(ltp)\n",
    "    inedx_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T06:55:49.744040Z",
     "start_time": "2021-06-19T05:29:17.693138Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_loss_list =[]\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for i in tqdm(range(18)):\n",
    "    this_model = f'./best_model_multi{i+1}.pt'\n",
    "    this_total_loss = 0.0\n",
    "    model = torch.load(this_model, map_location=torch.device('cpu'))\n",
    "    \n",
    "    inedx_counter = 0\n",
    "    df3['loss'] = 0.0\n",
    "    for one_inst in tqdm(df3.inst.astype(dtype='int32')):\n",
    "        _, val_data_ip = get_data2(one_inst)\n",
    "        this_loss = plot_and_loss(model, val_data_ip, 1, one_inst)\n",
    "        this_total_loss+=this_loss\n",
    "        df3.at[inedx_counter, 'loss'] = this_loss\n",
    "        inedx_counter+=1\n",
    "    print(this_model)\n",
    "    print(this_total_loss)\n",
    "        \n",
    "    model_loss_list.append({'model':this_model,'this_total_loss':this_total_loss})\n",
    "    \n",
    "model_loss_list_edf = pd.DataFrame(model_loss_list)\n",
    "model_loss_list_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T07:01:52.203907Z",
     "start_time": "2021-06-19T06:57:35.025268Z"
    }
   },
   "outputs": [],
   "source": [
    "this_model = f'./best_model_multi7.pt'\n",
    "this_total_loss = 0.0\n",
    "model = torch.load(this_model, map_location=torch.device('cpu'))\n",
    "\n",
    "inedx_counter = 0\n",
    "df3['loss'] = 0.0\n",
    "for one_inst in tqdm(df3.inst.astype(dtype='int32')):\n",
    "    _, val_data_ip = get_data2(one_inst)\n",
    "    this_loss = plot_and_loss(model, val_data_ip, 1, one_inst)\n",
    "    df3.at[inedx_counter, 'loss'] = this_loss\n",
    "    inedx_counter+=1\n",
    "\n",
    "print(this_model)\n",
    "print(this_total_loss)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(df3.corr())\n",
    "plt.show()\n",
    "df3['ltp_by_loss'] = df3['ltp']/df3['loss']\n",
    "# df3[['ltp_by_loss']]\n",
    "df3['ltp_by_lossx10'] = df3['ltp_by_loss']*20\n",
    "df3['lossx10'] = df3['loss']*20\n",
    "df3[['ltp','lossx10','ltp_by_lossx10']].plot()\n",
    "ax = df3[['ltp','lossx10','ltp_by_lossx10']].plot.hist(bins=100, alpha=0.3)\n",
    "df3[df3.ltp_by_loss > 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T07:01:52.975911Z",
     "start_time": "2021-06-19T07:01:52.393656Z"
    }
   },
   "outputs": [],
   "source": [
    "this_model = f'./best_model_multi3.pt'\n",
    "this_total_loss = 0.0\n",
    "model = torch.load(this_model, map_location=torch.device('cpu'))\n",
    "\n",
    "inedx_counter = 0\n",
    "df3['loss'] = 0.0\n",
    "for one_inst in tqdm(df3.inst.astype(dtype='int32')):\n",
    "    _, val_data_ip = get_data2(one_inst)\n",
    "    this_loss = plot_and_loss(model, val_data_ip, 1, one_inst)\n",
    "    df3.at[inedx_counter, 'loss'] = this_loss\n",
    "    inedx_counter+=1\n",
    "\n",
    "print(this_model)\n",
    "print(this_total_loss)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(df3.corr())\n",
    "plt.show()\n",
    "df3['ltp_by_loss'] = df3['ltp']/df3['loss']\n",
    "# df3[['ltp_by_loss']]\n",
    "df3['ltp_by_lossx10'] = df3['ltp_by_loss']*20\n",
    "df3['lossx10'] = df3['loss']*20\n",
    "df3[['ltp','lossx10','ltp_by_lossx10']].plot()\n",
    "ax = df3[['ltp','lossx10','ltp_by_lossx10']].plot.hist(bins=100, alpha=0.3)\n",
    "df3[df3.ltp_by_loss > 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T10:19:20.541518Z",
     "start_time": "2021-06-19T10:19:20.377117Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.to_excel('./df3.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T14:37:22.679724Z",
     "start_time": "2021-06-19T14:37:22.674147Z"
    }
   },
   "outputs": [],
   "source": [
    "import QuantConnect_Reserved"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "462px",
    "left": "995px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
